<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.wukongblog.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"waline","storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/./public/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="From Alice Vision.  NATURAL FEATURE EXTRACTION The objective of this step is to extract distinctive groups of pixels that are, to some extent, invariant to changing camera viewpoints during imag">
<meta property="og:type" content="article">
<meta property="og:title" content="三维重建流程(AilceVision)">
<meta property="og:url" content="https://www.wukongblog.com/posts/dd277f8c.html">
<meta property="og:site_name" content="WuKongBlog">
<meta property="og:description" content="From Alice Vision.  NATURAL FEATURE EXTRACTION The objective of this step is to extract distinctive groups of pixels that are, to some extent, invariant to changing camera viewpoints during imag">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-03-05T07:00:00.000Z">
<meta property="article:modified_time" content="2023-05-25T15:19:28.288Z">
<meta property="article:author" content="Wu Kong">
<meta property="article:tag" content="三维重建">
<meta property="article:tag" content="摄影测量">
<meta property="article:tag" content="计算机视觉">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://www.wukongblog.com/posts/dd277f8c.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://www.wukongblog.com/posts/dd277f8c.html","path":"posts/dd277f8c.html","title":"三维重建流程(AilceVision)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>三维重建流程(AilceVision) | WuKongBlog</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-GNGQMJ4BMM"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-GNGQMJ4BMM","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?90241e32c8029a7ce0c7e36ef300a89b"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">WuKongBlog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#natural-feature-extraction"><span class="nav-number">1.</span> <span class="nav-text">NATURAL FEATURE EXTRACTION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#image-matching"><span class="nav-number">2.</span> <span class="nav-text">IMAGE MATCHING</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#features-matching"><span class="nav-number">3.</span> <span class="nav-text">FEATURES MATCHING</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#structure-from-motion"><span class="nav-number">4.</span> <span class="nav-text">STRUCTURE FROM MOTION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#depth-maps-estimation"><span class="nav-number">5.</span> <span class="nav-text">DEPTH MAPS ESTIMATION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#meshing"><span class="nav-number">6.</span> <span class="nav-text">MESHING</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#texturing"><span class="nav-number">7.</span> <span class="nav-text">TEXTURING</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#localization"><span class="nav-number">8.</span> <span class="nav-text">LOCALIZATION</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#camera-calibration"><span class="nav-number">8.1.</span> <span class="nav-text">CAMERA CALIBRATION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#single-camera-localization"><span class="nav-number">8.2.</span> <span class="nav-text">SINGLE CAMERA LOCALIZATION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rig-of-cameras"><span class="nav-number">8.3.</span> <span class="nav-text">RIG OF CAMERAS</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Wu Kong"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Wu Kong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wongshek" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wongshek" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wong.shek@outlook.com" title="E-Mail → mailto:wong.shek@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://wukongblog-1258511654.cos.ap-guangzhou.myqcloud.com/PicGo/img/wechat.jpg" title="WeChat → https:&#x2F;&#x2F;wukongblog-1258511654.cos.ap-guangzhou.myqcloud.com&#x2F;PicGo&#x2F;img&#x2F;wechat.jpg" rel="noopener" target="_blank"><i class="fab fa-weixin fa-fw"></i>WeChat</a>
      </span>
  </div>



<div class="links-of-blogroll motion-element">
	<p id="hitokoto" style="padding: 0; margin: 0;">:D 获取中...</p>
	<p id="hitofrom" style="text-align: right; padding: 0; margin: 0;">:D 获取中...</p>
	<script>
        fetch('https://v1.hitokoto.cn/?c=i')
        .then(response => response.json())
        .then(data => {
            const hitokoto = document.getElementById('hitokoto')
            const hitofrom = document.getElementById('hitofrom')
            hitokoto.innerText = data.hitokoto
            hitofrom.innerText = '——' + data.from
        })
        .catch(console.error)
	</script>
</div>

<div class="revolver-maps">
	<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5ttrp7ce0jk&amp;m=6&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
</div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.wukongblog.com/posts/dd277f8c.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Wu Kong">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WuKongBlog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="三维重建流程(AilceVision) | WuKongBlog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          三维重建流程(AilceVision)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-03-05 15:00:00" itemprop="dateCreated datePublished" datetime="2021-03-05T15:00:00+08:00">2021-03-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-05-25 23:19:28" itemprop="dateModified" datetime="2023-05-25T23:19:28+08:00">2023-05-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" itemprop="url" rel="index"><span itemprop="name">基础知识</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <div class="note info"><p>From <a target="_blank" rel="noopener" href="https://alicevision.org/#photogrammetr">Alice
Vision</a>.</p>
</div>
<h2 id="natural-feature-extraction">NATURAL FEATURE EXTRACTION</h2>
<p>The objective of this step is to extract distinctive groups of pixels
that are, to some extent, invariant to changing camera viewpoints during
image acquisition. Hence, a feature in the scene should have similar
feature descriptions in all images.</p>
<span id="more"></span>
<p>The most well-know feature detection method is the SIFT
(Scale-invariant feature transform) algorithm. The initial goal of SIFT
is to extract discriminative patches in a first image that can be
compared to discriminative patches of a second image irrespective of
rotation, translation, and scale. As a relevant detail only exists at a
certain scale, the extracted patches are centered at stable points of
interest. The key idea is that, to some extent, one can use the SIFT
invariance to deal with the image transformations occurring when the
viewpoints are changing during image acquisition.</p>
<p>From the representation of one image at different scales, which is
technically done by computing a pyramid of downscaled images. SIFT
computes scale-space maxima of the Laplacian representation, which is a
specific image energy-based representation of the image, using so-called
differences of Gaussians. These maxima correspond to points of interest.
It then samples for each one of these maxima a square image patch whose
origin is the maximum and x-direction is the dominant gradient at the
origin. For each keypoint, a description of these patches is
associated.</p>
<p>The description, which is typically stored in 128 bits, consists of a
statistics of gradients computed in regions around the keypoint. The
region size is determined by the keypoint scale and the orientation is
determined by the dominant axis.</p>
<p>As the number of extracted features may vary a lot due to the
variability of textures complexity (from one image to another or in
different parts of the image), a post-filtering step is used to control
the number of extracted features to reasonable limits (for instance
between one and ten thousands per image). We use a grid filtering to
ensure a good repartition in the image.</p>
<h2 id="image-matching">IMAGE MATCHING</h2>
<p>The objective of this part is to find images that are looking to the
same areas of the scene. For that, we use the image retrieval techniques
to find images that share some content without the cost of resolving all
feature matches in details. The ambition is to simplify the image in a
compact image descriptor which allows to compute the distance between
all images descriptors efficiently.</p>
<p>One of the most common method to generate this image descriptor is
the vocabulary tree approach. By passing all extracted features
descriptors into it, it makes a classification by comparing their
descriptors to the ones on each node of this tree. Each feature
descriptor ends up in one leaf, which can be stored by a simple index:
the index of this leaf in the tree. The image descriptor is then
represented by this collection of used leaves indices.</p>
<p>It is now possible to see if different images share the same content
by comparing these image descriptors.</p>
<h2 id="features-matching">FEATURES MATCHING</h2>
<p>The objective of this step is to match all features between candidate
image pairs.</p>
<p>First, we perform photometric matches between the set of descriptors
from the 2 input images. For each feature in image A, we obtain a list
of candidate features in image B. As the descriptor space is not a
linear and well defined space, we cannot rely on absolute distance
values to know if the match is valid or not (we can only have an
absolute higher bound distance). To remove bad candidates, we assume
that there’s only one valid match in the other image. So for each
feature descriptor on the first image, we look for the 2 closest
descriptors and we use a relative threshold between them. This
assumption will kill features on repetitive structure but has proved to
be a robust criterion [Lowe2004]. This provide a list of feature
matching candidates based only on a photometric criterion. Find the 2
closest descriptors in the second image for each feature is
computationally intensive with a brute force approach, but many
optimized algorithms exists. The most common one is Approximate Nearest
Neighbor, but there are alternatives like, Cascading Hashing.</p>
<p>Then, we use the features positions in the images to make a geometric
filtering by using epipolar geometry in an outlier detection framework
called RANSAC (RANdom SAmple Consensus). We randomly select a small set
of feature correspondences and compute the fundamental (or essential)
matrix, then we check the number of features that validates this model
and iterate through the RANSAC framework.</p>
<h2 id="structure-from-motion">STRUCTURE FROM MOTION</h2>
<p>The objective of this step is to understand the geometric
relationship behind all the observations provided by the input images,
and infer the rigid scene structure (3D points) with the pose (position
and orientation) and internal calibration of all cameras. The
Incremental pipeline is a growing reconstruction process. It first
computes an initial two-view reconstruction that is iteratively extended
by adding new views.</p>
<iframe width="100%" height="450" src="https://sketchfab.com/models/31c093dca5a84057983d8de22a96b416/embed" frameborder="0" allowvr allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" onmousewheel style="box-sizing: border-box;">
</iframe>
<p>First, it fuses all feature matches between image pairs into tracks.
Each track is supposed to represent a point in space, visible from
multiple cameras. However, at this step of the pipeline, it still
contains many outliers. During this fusion of matches, we remove
incoherent tracks.</p>
<p>Then, the incremental algorithm has to choose the best initial image
pair. This choice is critical for the quality of the final
reconstruction. It should indeed provide robust matches and contain
reliable geometric information. So, this image pair should maximize the
number of matches and the repartition of the corresponding features in
each image. But at the same time, the angle between the cameras should
also be large enough to provide reliable geometric information.</p>
<p>Then we compute the fundamental matrix between these 2 images and
consider that the first one is the origin of the coordinate system. Now
that we know the pose of the 2 first cameras, we can triangulate the
corresponding 2D features into 3D points.</p>
<p>After that, we select all the images that have enough associations
with the features that are already reconstructed in 3D. This algorithm
is called next best views selection. Based on these 2D-3D associations
it performs the resectioning of each of these new cameras. The
resectioning is a Perspective-n-Point algorithm (PnP) in a RANSAC
framework to find the pose of the camera that validates most of the
features associations. On each camera, a non-linear minimization is
performed to refine the pose.</p>
<p>From these new cameras poses, some tracks become visible by 2 or more
resected cameras and it triangulates them. Then, we launch a Bundle
Adjustment to refine everything: extrinsics and intrinsics parameters of
all cameras as well as the position of all 3D points. We filter the
results of the Bundle Adjustment by removing all observations that have
high reprojection error or insufficient angles between observations.</p>
<p>As we have triangulated new points, we get more image candidates for
next best views selection. We iterate like that, adding cameras and
triangulating new 2D features into 3D points and removing 3D points that
became invalidated, until we can’t localize new views.</p>
<p>Many other approaches exists like Global [Moulon2013], Hierarchical
[Havlena2010], [Toldo2015] or multi-stage [Shah2014] approaches.</p>
<h2 id="depth-maps-estimation">DEPTH MAPS ESTIMATION</h2>
<p>For all cameras that have been resolved by SfM, we want to retrieve
the depth value of each pixel. Many approaches exist, like Block
Matching, Semi-Global Matching (SGM) [Hirschmüller2005],
[Hirschmüller2008] or ADCensus [Xing2011]. We will focus on the SGM
method implemented in AliceVision.</p>
<p>For each image, we select the N best/closest cameras around. We
select fronto-parallel planes based on the intersection of the optical
axis with the pixels of the selected neighboring cameras. This creates a
volume W, H, Z with many depth candidates per pixel. We estimate the
similarity for all of them. The similarity is computed by the Zero Mean
Normalized Cross-Correlation (ZNCC) of a small patch in the main image
reprojected into the other camera. This create a volume of similarities.
For each neighboring image, we accumulate similarities into this volume.
This volume is very noisy. We apply a filtering step along X and Y axes
which accumulates local costs which drastically reduce the score of
isolated high values. We finally select the local minima and replace the
selected plane index with the depth value stored into a depth map. This
depth map has banding artifacts as it is based on the original selection
of depth values. So a refine step is applied to get depth values with
sub-pixel accuracy.</p>
<p>All these depth maps can be computed independently in parallel. Then
we apply a filtering step to ensure consistency between multiple
cameras. A compromise is chosen based on both similarity value and the
number of coherent cameras to keep weakly supported surfaces without
adding artefacts.</p>
<h2 id="meshing">MESHING</h2>
<p>The objective of this step is to create a dense geometric surface
representation of the scene.</p>
<p>First, we fuse all the depth maps into a global octree where
compatible depth values are merged into the octree cells.</p>
<p>We then perform a 3D Delaunay tetrahedralization. Then a complex
voting procedure is done to compute weights on cells and weights on
facets connecting the cells as explained in [Jancosek2011] and
[Jancosek2014].</p>
<p>A Graph Cut Max-Flow [Boykov2004] is applied to optimally cut the
volume. This cut represents the extracted mesh surface. We filter bad
cells on the surface. We finally apply a Laplacian filtering on the mesh
to remove local artefacts.</p>
<p>At this point, the mesh can also be simplified to reduce unnecessary
vertices.</p>
<h2 id="texturing">TEXTURING</h2>
<p>The objective of this step is to texture the generated mesh.</p>
<iframe width="100%" height="450" src="https://sketchfab.com/models/0d38fbbf35ce44d085e2ec18368942df/embed" frameborder="0" allowvr allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" onmousewheel style="box-sizing: border-box;">
</iframe>
<p>If the mesh has no associated UV, it computes automatic UV maps.
AliceVision implements a basic UV mapping approach to minimize the
texture space. The standard UV mapping approach is provided by
[Levy2002].</p>
<p>For each triangle, we use the visibility information associated to
each vertex to retrieve the texture candidates. We filter the cameras
without a good angle to the surface to favor fronto-parallel cameras and
finally average the pixel values. Instead of a naive averaging, we use a
generalization of the multi-band blending described in [Burt1983], so we
average more views in the low frequencies than in the high frequencies.
This approach is in the same spirit than [Baumberg2002] and
[Allene2008].</p>
<h2 id="localization">LOCALIZATION</h2>
<p>Based on the SfM results, we can perform camera localization and
retrieve the motion of an animated camera in the scene of the 3D
reconstruction.</p>
<h3 id="camera-calibration">CAMERA CALIBRATION</h3>
<p>The internal camera parameters can be calibrated from multiple views
of a checkerboard. This allows to retrieve focal length, principal point
and distortion parameters. A detailed explanation is presented in
[opencvCameraCalibration].</p>
<h3 id="single-camera-localization">SINGLE CAMERA LOCALIZATION</h3>
<p>We use the algorithm presented in image matching section to localize
the closest images in the SfM results. Then we perform feature matching
with those images as well as with the N previous frames. Then we
directly get 2D-3D associations, that are used to localize the camera.
Finally, a global Bundle Adjustment is performed to refine the camera
parameters.</p>
<h3 id="rig-of-cameras">RIG OF CAMERAS</h3>
<p>If a rig of cameras is used, we can perform the rig calibration. We
localize cameras individually on the whole sequence. Then we use all
valid poses to compute the relative poses between cameras of the rig and
choose the more stable value across the images. Then we initialize the
rig relative pose with this value and perform a global Bundle Adjustment
on all the cameras of the rig. When the rig is calibrated, we can use it
to directly localize the rig pose from the synchronized multi-cameras
system with [Kneip2014] approaches.</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Wu Kong
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://www.wukongblog.com/posts/dd277f8c.html" title="三维重建流程(AilceVision)">https://www.wukongblog.com/posts/dd277f8c.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" rel="tag"># 三维重建</a>
              <a href="/tags/%E6%91%84%E5%BD%B1%E6%B5%8B%E9%87%8F/" rel="tag"># 摄影测量</a>
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"># 计算机视觉</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/6fdca1fc.html" rel="prev" title="简单影像增强">
                  <i class="fa fa-chevron-left"></i> 简单影像增强
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/c6338439.html" rel="next" title="C++使用Matplotlib画图">
                  C++使用Matplotlib画图 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">  Wu Kong</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
